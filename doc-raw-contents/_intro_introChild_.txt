contrary to popular belief, lorem ipsum is not simply random text. it has roots in a piece of classical latin literature from 45 bc, making it over 2000 years old. richard mcclintock, a latin professor at hampden-sydney college in virginia, looked up one of the more obscure latin words, consectetur, from a lorem ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. lorem ipsum comes from sections 1.10.32 and 1.10.33 of “de finibus bonorum et malorum” (the extremes of good and evil) by cicero, written in 45 bc. this book is a treatise on the theory of ethics, very popular during the renaissance. the first line of lorem ipsum, “lorem ipsum dolor sit amet.”, comes from a line in section 1.10.32. the model is trained and optimized for greater-than-word length text, such as sentences, phrases or short paragraphs. it is trained on a variety of data sources and a variety of tasks with the aim of dynamically accommodating a wide variety of natural language understanding tasks. the input is variable length english text and the output is a 512 dimensional vector. we apply this model to the sts benchmark for semantic similarity, and the results can be seen in the example notebook made available. the universal-sentence-encoder model is trained with a deep averaging network (dan) encoder. to learn more about text embeddings, refer to the tensorflow embeddings documentation. our encoder differs from word level embedding models in that we train on a number of natural language prediction tasks that require modeling the meaning of word sequences rather than just individual words. details are available in the paper “universal sentence encoder” fsdfs